{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxm7RNOj-vaZ"
   },
   "source": [
    "# DAY 1 - Generative Artificial Intelligence\n",
    "**A simple neural network to shows how the XOR problem can be solved with neural networks when its parameters are set by hand.**\n",
    "\n",
    "This imports the NumPy library and gives it the alias np. NumPy provides efficient array and matrix operations, which are essential for neural network computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MgUVEp13-jae"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmB_oIQzMj3J"
   },
   "source": [
    "This defines a function phi(x) that acts as a simple threshold (step) activation function:\n",
    "* np.greater_equal(x, 1) compares every element of x to 1 and returns:\n",
    "  * True if the element is greater than or equal to 1\n",
    "  * False otherwise.\n",
    "* .astype(int) converts True → 1 and False → 0.\n",
    "\n",
    "So overall: phi(x) returns 1 if x ≥ 1, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ECsixvMzNQz3"
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "  return np.greater_equal(x,1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRAaawEDNZAu"
   },
   "source": [
    "This defines a two-layer neural network and computes the forward propagation:\n",
    "* nn(x, w1, w2) where\n",
    "  * x is the input vector.\n",
    "  * w1 is the weight matrix connecting the input layer to the hidden layer.\n",
    "  * w2 is the weight matrix connecting the hidden layer to the output layer.\n",
    "\n",
    "* np.dot(x, w1) performs the dot product between input x and weight matrix w1, giving the hidden layer's pre-activation values, and applies phi() applies the threshold function to get the hidden layer's activations (0 or 1).\n",
    "\n",
    "* np.dot(h1, w1) takes the dot product of the hidden activations h1 and the second weight matrix w2, and applies phi() again to produce the output (0 or 1).\n",
    "\n",
    "Returns the final output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LlKqq8mrNp0l"
   },
   "outputs": [],
   "source": [
    "def nn(x, w1, w2):\n",
    "  h1 = phi(np.dot(x, w1))\n",
    "  y = phi(np.dot(h1, w2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5ICFSLQy4_"
   },
   "source": [
    "These define:\n",
    "* w1: a 2x3 matrix (2 input neurons → 3 hidden neurons).\n",
    "* w2: a 3x1 matrix (3 hidden neurons → 1 output neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cr5YpwZFQ_0-"
   },
   "outputs": [],
   "source": [
    "w1 = np.array([ [1, 0.5, 0], [0, 0.5, 1] ])\n",
    "w2 = np.array([[1], [-2], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfXOIs04RGpJ"
   },
   "source": [
    "Test the neural network with different inputs and see that it indeed solves the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1761150592993,
     "user": {
      "displayName": "Laurent Cretegny",
      "userId": "04229628776398951952"
     },
     "user_tz": -120
    },
    "id": "Hq7Vl48-RM_w",
    "outputId": "5aeb665e-bd73-4056-cfc2-0b0151539cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(nn([1,0], w1, w2))\n",
    "print(nn([0,1], w1, w2))\n",
    "print(nn([0,0], w1, w2))\n",
    "print(nn([1,1], w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5An0giCYnaD"
   },
   "source": [
    "**Training a neural network for the XOR problem so that its parameters can be estimated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6nuDT61ZAcC"
   },
   "source": [
    "Define an activation function that is differentiable (this is essential for computing gradients during backpropagation). The sigmoid function maps any real number to the range (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "12pMZcyEZfa5"
   },
   "outputs": [],
   "source": [
    "def phi(x, deriv = False):\n",
    "    if deriv:\n",
    "        return phi(x)*(1-phi(x))\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa9G-d8hav1i"
   },
   "source": [
    "This computes the forward propagation — i.e., how the input flows through the network to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sYDA3pY2a1C5"
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, w1, w2):\n",
    "    h1 = phi(x.dot(w1))\n",
    "    y = phi(h1.dot(w2))\n",
    "    return h1, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUwNvCWbdA-y"
   },
   "source": [
    "This function performs backpropagation — computing gradients of the loss with respect to the weights, and updating the weights to minimize that loss.\n",
    "* gold is the target.\n",
    "* lrate is the learning rate (controls the size of each step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4IzpqU5odVAZ"
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, gold, w1, w2, h1, y, lrate):\n",
    "# Computes the error at the output layer (diff. between prediction and target).\n",
    "    l2_error = y - gold\n",
    "# Multiplies the output error by the derivative of the activation function at\n",
    "# the output.This gives the gradient of the loss w.r.t. the output layer input\n",
    "# (pre-activation).\n",
    "    l2_deriv = l2_error*phi(y,deriv=True)\n",
    "# Propagates the error backward from the output layer to the hidden layer. This\n",
    "# tells us how much each hidden neuron contributed to the final error.\n",
    "    l1_error = l2_deriv.dot(w2.T)\n",
    "# Multiplies the hidden layer error by the derivative of the activation\n",
    "# function. This gives the gradient of the loss w.r.t. the hidden layer input\n",
    "# (pre-activation).\n",
    "    l1_deriv = l1_error*phi(h1,deriv=True)\n",
    "# Multiply the input (transposed) by error gradient to get the weight gradient\n",
    "# Subtract this scaled gradient from the current weights (gradient descent step)\n",
    "    w2 -= lrate*h1.T.dot(l2_deriv)\n",
    "    w1 -= lrate*x.T.dot(l1_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liGM9xJLMe3Y"
   },
   "source": [
    "Define the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rQRHrMoekZ--"
   },
   "outputs": [],
   "source": [
    "x = np.array([ [0, 1], [1, 1], [1, 0], [0, 0] ])\n",
    "y_true = np.array([ [1], [0], [1], [0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW3l3rYHktI6"
   },
   "source": [
    "Initialize randomly model parameters in the define network and train it for some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1761150594011,
     "user": {
      "displayName": "Laurent Cretegny",
      "userId": "04229628776398951952"
     },
     "user_tz": -120
    },
    "id": "Uq7nCJPhlUbw",
    "outputId": "b521d66e-a878-44ef-e135-2b0696a7d6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 24.47181639   1.58626583 -22.39277136]\n",
      " [-22.9019011    1.6651065   23.97162847]] \n",
      "\n",
      "[[-18.70623044]\n",
      " [ 27.76155203]\n",
      " [-19.01096321]] \n",
      "\n",
      "[[0.9870592 ]\n",
      " [0.01078543]\n",
      " [0.98711037]\n",
      " [0.00684192]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.random((2,3))\n",
    "w2 = np.random.random((3,1))\n",
    "\n",
    "for j in range(10000):\n",
    "    h1, y = forward_pass(x, w1, w2)\n",
    "    backward_pass(x, y_true, w1, w2, h1 , y , lrate = 1)\n",
    "\n",
    "print(w1, '\\n')\n",
    "print(w2, '\\n')\n",
    "print(forward_pass(x, w1, w2)[1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5g1mR4cMe3Z"
   },
   "source": [
    "**High-level code with neural network Library**\n",
    "\n",
    "Neural network libraries like [PyTorch](https://pytorch.org/) make it much easier to define and train large-scale networks:\n",
    "\n",
    "- libraries provide implementations of common building blocks (neural layers, activation functions, loss functions, ...). This saves development effort and increases readability.\n",
    "- you only need to define the forward pass; the backward pass required to determine gradients is built automatically (autodiff).\n",
    "- under the hood, building blocks have optimized implementations for different hardware (CPU/GPU) -- GPU support is crucial for large-scale neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XeSvLhoqjFU"
   },
   "source": [
    "Bring in PyTorch (torch), the neural-network layers/utilities (torch.nn), and optimization algorithms (torch.optim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f5KYRL4-qgrk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MXJdU8rIxt"
   },
   "source": [
    "Define the model.\n",
    "- Creates a subclass of nn.Module called Net.\n",
    "- super(Net, self).\\_\\_init\\_\\_() initializes base nn.Module state.\n",
    "  - self.ff1 = nn.Linear(2, 3, bias=False): first fully connected layer: input size 2 → hidden size 3, no bias. Its weight matrix has shape (3, 2).\n",
    "  - self.ff2 = nn.Linear(3, 1, bias=False): second fully connected layer: hidden 3 → output 1, no bias. Weight matrix shape (1, 3).\n",
    "\n",
    "Define the forward pass.\n",
    "- Passes x through ff1, then applies the sigmoid activation element-wise (outputs in (0,1)).\n",
    "- Passes the hidden activations through ff2, then another sigmoid to produce a scalar probability-like output per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6CPo47MErm-K"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ff1 = nn.Linear(2, 3, bias=False)\n",
    "        self.ff2 = nn.Linear(3, 1, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.ff1(x))\n",
    "        x = torch.sigmoid(self.ff2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khXfrdr5r_et"
   },
   "source": [
    "Train the model by defining input and ouptut data.\n",
    "- x: a batch of 4 input vectors with 2 features each.\n",
    "- y_true: the corresponding targets (here, this matches the XOR truth table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "70U62ZkjsMj1"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([ [0, 1], [1, 1], [1, 0], [0, 0] ],\n",
    "            dtype=torch.float)\n",
    "y_true = torch.tensor([ [1], [0], [1], [0] ],\n",
    "            dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RImRfy1esi7L"
   },
   "source": [
    "Instantiate the network and inspect initial output.\n",
    "- Creates the model with randomly initialized weights.\n",
    "- Runs a forward pass on the whole batch to see the initial predictions before training.\n",
    "- Prints those initial predictions (values in (0,1) due to sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1761150600065,
     "user": {
      "displayName": "Laurent Cretegny",
      "userId": "04229628776398951952"
     },
     "user_tz": -120
    },
    "id": "_lMDgaals1gy",
    "outputId": "3fb1a715-792a-42ae-e92f-890e76bcc536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4254],\n",
      "        [0.4325],\n",
      "        [0.4158],\n",
      "        [0.4076]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "output = net(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5-4jBLes5Cq"
   },
   "source": [
    "Loss and optimizer.\n",
    "- criterion: mean squared error loss between predictions and targets.\n",
    "- optimizer: stochastic gradient descent over all model parameters with learning rate 1 (quite large, but workable in this tiny example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "StKW9FB9tX63"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ji4NFDrtCJ6"
   },
   "source": [
    "Training loop. For each of 5000 iterations:\n",
    "- optimizer.zero_grad(): clears any accumulated gradients from the previous step (PyTorch accumulates by default).\n",
    "- output = net(x): runs a forward pass to compute current predictions with the latest weights.\n",
    "- loss = criterion(output, y_true): computes the MSE loss over the batch.\n",
    "- loss.backward(): uses autograd to compute gradients of loss w.r.t. all parameters in net.\n",
    "- optimizer.step(): applies SGD to update the weights using those gradients (one step of gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MloNKHy2tTD9"
   },
   "outputs": [],
   "source": [
    "for j in range(5000):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(x) # forward pass\n",
    "    loss = criterion(output, y_true) # compute loss\n",
    "    loss.backward()     # backward pass to get gradients\n",
    "    optimizer.step()    # update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu4sNbdNtgA0"
   },
   "source": [
    "Evaluate after training.\n",
    "- Runs one more forward pass with the trained weights.\n",
    "- Prints the final predictions, which (for XOR) should be close to [[1],[0],[1],[0]] (e.g., ~0.99 vs ~0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1761150611325,
     "user": {
      "displayName": "Laurent Cretegny",
      "userId": "04229628776398951952"
     },
     "user_tz": -120
    },
    "id": "5aKuDiuItsUA",
    "outputId": "cb3102ab-ed5b-4482-be38-44069f382f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9313],\n",
      "        [0.0682],\n",
      "        [0.9155],\n",
      "        [0.0503]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOdY2ZGmTr2IPiQg3qYQVb7",
   "provenance": [
    {
     "file_id": "1GMenxODPWTvDBuaXGXgjgcebh5kgqRfF",
     "timestamp": 1761150651091
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
